from airflow import DAG
#from airflow.providers.amazon.aws.operators.emr_create_job_flow import EmrCreateJobFlowOperator
#from airflow.providers.amazon.aws.operators.emr_add_steps import EmrAddStepsOperator
#from airflow.providers.amazon.aws.operators.emr_terminate_job_flow import EmrTerminateJobFlowOperator
#from airflow.providers.amazon.aws.sensors.emr_step import EmrStepSensor
#from airflow.operators.dummy_operator import DummyOperator
from datetime import datetime, timedelta
from airflow.providers.amazon.aws.operators.emr import EmrCreateJobFlowOperator
from airflow.providers.amazon.aws.operators.emr import EmrAddStepsOperator
from airflow.providers.amazon.aws.operators.emr import EmrTerminateJobFlowOperator
from airflow.providers.amazon.aws.sensors.emr import EmrStepSensor

default_args = {
    'start_date': datetime(2023, 6, 12),
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# EMR cluster configuration
emr_cluster_config = {
    'Name': 'SparkCluster',
    'ReleaseLabel': 'emr-6.3.0',
    'LogUri': 's3://your-bucket/logs/',
    'Instances': {
        'InstanceGroups': [
            {
                'Name': 'Master node',
                'Market': 'SPOT',
                'InstanceRole': 'MASTER',
                'InstanceType': 'm5.xlarge',
                'InstanceCount': 1,
            },
            {
                'Name': 'Core nodes',
                'Market': 'SPOT',
                'InstanceRole': 'CORE',
                'InstanceType': 'm5.xlarge',
                'InstanceCount': 2,
            },
        ],
        'Ec2KeyName': 'your-key-pair',
        'KeepJobFlowAliveWhenNoSteps': False,
        'TerminationProtected': False,
    },
    'Applications': [
        {
            'Name': 'Spark'
        },
    ],
}

# Spark job step configuration
spark_job_step = {
    'Name': 'Spark Job',
    'ActionOnFailure': 'CONTINUE',
    'HadoopJarStep': {
        'Jar': 'command-runner.jar',
        'Args': [
            'spark-submit',
            '--deploy-mode', 'cluster',
            '--master', 'yarn',
            's3://script-bucket/myscript.py',
        ],
    },
}

# DAG definition
dag = DAG(
    'emr_spark_processing',
    description='DAG to start EMR cluster, run Spark job, and terminate cluster',
    schedule_interval=None,
    default_args=default_args,
    catchup=False
)

# Start EMR Cluster
start_cluster = EmrCreateJobFlowOperator(
    task_id='start_emr_cluster',
    aws_conn_id='aws_default',
    emr_conn_id='aws_default',
    job_flow_overrides=emr_cluster_config,
    dag=dag
)

# Execute Spark job on EMR Cluster
execute_spark_job = EmrAddStepsOperator(
    task_id='execute_spark_job',
    job_flow_id="{{ task_instance.xcom_pull(task_ids='start_emr_cluster', key='return_value') }}",
    aws_conn_id='aws_default',
    steps=[spark_job_step],
    dag=dag
)

# Wait for Spark job completion
wait_for_completion = EmrStepSensor(
    task_id='wait_for_completion',
    job_flow_id="{{ task_instance.xcom_pull(task_ids='start_emr_cluster', key='return_value') }}",
    step_id="{{ task_instance.xcom_pull(task_ids='execute_spark_job', key='return_value')[0] }}",
    aws_conn_id='aws_default',
    dag=dag
)

# Terminate EMR Cluster
terminate_cluster = EmrTerminateJobFlowOperator(
task_id='terminate_emr_cluster',
job_flow_id="{{ task_instance.xcom_pull(task_ids='start_emr_cluster', key='return_value') }}",
aws_conn_id='aws_default',
dag=dag
)

start_cluster >> execute_spark_job >> wait_for_completion >> terminate_cluster
